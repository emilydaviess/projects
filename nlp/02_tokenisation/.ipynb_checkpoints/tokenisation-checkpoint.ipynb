{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40adb7e-8ec2-4dcc-bf15-2d79ec3c04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ad557f-884c-4489-95f0-ef66d452a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n",
      "token: '\n",
      "token: Let\n",
      "token: 's\n",
      "token: go\n",
      "token: to\n",
      "token: N.Y.C\n",
      "token: !\n",
      "token: '\n"
     ]
    }
   ],
   "source": [
    "# when you call spacy.blank(\"de\"), it creates a blank German (\"de\" for Deutsch) language model. \n",
    "# this means that the model will have no pipeline components (like tagger, parser, entity recognizer) pre-loaded.\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"'Let's go to N.Y.C!'\")\n",
    "print(type(nlp))\n",
    "for token in doc:\n",
    "    print(\"token:\",token)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d27cdc-6bb1-4cd8-b1fb-b3ee65e51249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Dr.\n",
      "token: Strange\n",
      "token: loves\n",
      "token: pav\n",
      "token: bhaji\n",
      "token: of\n",
      "token: mumbai\n",
      "token: as\n",
      "token: its\n",
      "token: costs\n",
      "token: only\n",
      "token: £\n",
      "token: 2\n",
      "token: per\n",
      "token: plate\n",
      "token: .\n"
     ]
    }
   ],
   "source": [
    "# when you call spacy.blank(\"de\"), it creates a blank German (\"de\" for Deutsch) language model. \n",
    "# this means that the model will have no pipeline components (like tagger, parser, entity recognizer) pre-loaded.\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as its costs only £2 per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"token:\",token)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aab92c4e-6b3f-4e73-9212-f945c589c9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)\n",
    "token=doc[1]\n",
    "print(token)\n",
    "token.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37aa7b4b-00a7-428f-87da-e2f1b957b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' --> index: 0 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "Let --> index: 1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "'s --> index: 2 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "go --> index: 3 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "to --> index: 4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "N.Y.C --> index: 5 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "! --> index: 6 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "' --> index: 7 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc: \n",
    "    print(token, \"-->\", \"index:\", token.i, \n",
    "          \"is_alpha:\", token.is_alpha, \"is_punct:\", token.is_punct, \"like_num:\", token.like_num, \"is_currency:\", token.is_currency\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0e487d5-c149-464a-acfc-b860db978f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name | Email \\n', 'Emily | emily@email.com\\n', 'Ashton | ashton@email.com\\n', 'Billie | bille@email.com']\n",
      "Name | Email \n",
      " Emily | emily@email.com\n",
      " Ashton | ashton@email.com\n",
      " Billie | bille@email.com\n",
      "emails: ['emily@email.com', 'ashton@email.com', 'bille@email.com']\n"
     ]
    }
   ],
   "source": [
    "# we're going to read a text file and find the emails using spacy.. we could do this via regex by spacy has inbuilt functionality to search for emails in text\n",
    "with open(\"students.txt\") as f: \n",
    "    text = f.readlines()\n",
    "print(text)\n",
    "\n",
    "text = ' '.join(text)\n",
    "print(text)\n",
    "\n",
    "emails = []\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "print(\"emails:\",emails)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01d66d98-4b15-4bef-ad9e-8811e2225d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'doube', 'cheese', 'extra', 'large', 'healthy', 'pizza']\n",
      "['gim', 'me', 'doube', 'cheese', 'extra', 'large', 'healthy', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# adding special rules into tokenizer\n",
    "# slang words - we want our nlp model spacy to recognise slang words like gimme - that should be two words 'give me'\n",
    "doc = nlp(\"gimme doube cheese extra large healthy pizza\") \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "\n",
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[{ORTH: \"gim\"},{ORTH: \"me\"}]) # we can't split to characters that do not already exist in the text\n",
    "doc = nlp(\"gimme doube cheese extra large healthy pizza\") \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f47aaece-28b0-495c-9db7-1d05d9dcf371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Dr. Strange loves pav bhaji of mumbai as its costs only £2 per plate.\n",
      "Hulk love chat of delhi\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as its costs only £2 per plate. Hulk love chat of delhi\")\n",
    "print(nlp.pipe_names)\n",
    "# for sentence in doc.sents:\n",
    "#     print(sentence)\n",
    "\n",
    "# this gives an error because we have created a 'blank' model and hence we don't have ability to break down by sentences, only tokens\n",
    "# we need to add a pipeline 'sentencizer'\n",
    "nlp.add_pipe('sentencizer')\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as its costs only £2 per plate. Hulk love chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b568c-3850-4aff-8aca-2ad6eabc7d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
