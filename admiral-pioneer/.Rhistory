results_df <- rbind(results_df, result_model)
print("yyyy")
predictions_df <- rbind(predictions_df, prediction)
print(predictions_df)
print("xxx")
}
}
}
}
}
}
}
}
predictions_df
View(predictions_df)
run_num <- 1
#Set Hyperparams:
nodes1 = c(256, 512, 1024)
nodes2 = c(64, 128, 256, 1024)
dropout1 = c(0.4, 0.2)
dropout2 = c(0.2)
optimizer = c("adam")
lr_annealing = c(0.001)
batch_size = c(50)
epochs = c(20)
combinations <- length(nodes1) * length(nodes2) * length(dropout1) *
length(dropout2) * length(optimizer) * length(lr_annealing) * length(batch_size) * length(epochs)
#Test different variations of hyperparameters using model_tune function from model_source.R!
results_df <- data.frame()
predictions_df <- data.frame()
run_num <- 1
for (n1 in nodes1){
for (n2 in nodes2){
for(d1 in dropout1){
for (d2 in dropout2){
for (opt in optimizer){
for (lr in lr_annealing){
for (bs in batch_size){
for (e in epochs){
print(paste("Run number ",run_num,"/",combinations, sep=""))
print(paste(n1,n2,d1,d2,opt,lr,bs,e))
source('model_train_nn.R') #model_train(train_data) function which will train our train dataset.
result <- model_train(policy_train,policy_test,n1,n2,d1,d2,opt,bs,e, lr)
result_model <- as.data.frame(result[1]) #results of model
prediction <- as.data.frame(result[2]) #predictions
results_df <- rbind(results_df, result_model)
predictions_df <- rbind(predictions_df, prediction)
run_num <- run_num + 1
}
}
}
}
}
}
}
}
results_df
View(results_df)
results_df <- results_df[order(-results_df$success_rate_claimers_25),]
opt_result <- results_df[1,] ## insert to agg_model_hyperparams
opt_result
opt_run_num <- opt_result$run_num[1]
opt_run_num
claim_predictions <- predictions_df[predictions_df$run_num == opt_run_num,]
View(claim_predictions)
#Using  50% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
#Using  25% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct_25 == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct_25 == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
View(claim_predictions)
claim_predictions <- claim_predictions[order(-claim_predictions$actual, -claim_predictions$prob),]
View(claim_predictions)
setwd("~/Documents/Emilys Docs/Projects/Admiral")
setwd("~/Documents/Emilys Docs/Projects/Admiral")
##################
#Load Libraries#
##################
library(DBI)
library(RSQLite)
library(dplyr)
library(woeBinning)
library(ggplot2)
library(C50)
library(mltools)
library(data.table)
library(gmodels)
set.seed(101)
options(scipen=999)
'%!in%' <- function(x,y)!('%in%'(x,y))
#############################
#DATA PULL & Data Cleaning#
############################
#Source in the load_data function from the file data_pull.R.
#This keeps the main script (model_source.R clean and easy to read.
source('0_data_pull.R') #load_data(database) function which will pull data from SQLite DB
policy_data <- load_data_dt("admiral_db.db") #Returns list of two, first is train data, second is test.
##################
#Load Libraries#
##################
library(DBI)
library(RSQLite)
library(dplyr)
library(woeBinning)
library(ggplot2)
library(C50)
library(mltools)
library(data.table)
library(gmodels)
set.seed(101)
options(scipen=999)
'%!in%' <- function(x,y)!('%in%'(x,y))
#############################
#DATA PULL & Data Cleaning#
############################
#Source in the load_data function from the file data_pull.R.
#This keeps the main script (model_source.R clean and easy to read.
source('1_data_pull.R') #load_data(database) function which will pull data from SQLite DB
policy_data <- load_data_dt("admiral_db.db") #Returns list of two, first is train data, second is test.
policy_train <- as.data.frame(policy_data[1])
policy_test <- as.data.frame(policy_data[2])
str(policy_train)
###################
#Model Build
###################
#1. Response Variable is classifed as CATEGORICAL - ClaimFlag (1/0).
#2. Model will be supervised, as we will train on labelled data.
#3. Chosen model is a descision tree.
#I've chosed this model, over the likes of a neural network/random forest because I want the model to be explainable to the customer.
#Therefore, if we were to use this model to predict the liklihood of somebody
str(policy_train)
response_index <- grep("ClaimFlag", colnames(policy_train))
fit <- C5.0(x = policy_train[,-response_index], y = policy_train[,c("ClaimFlag")])
summary(fit)
#The Error field of our model shows a rate of error of 12.6%
claim_pred <- predict(fit, policy_test[,-response_index])
claim_prob <- predict(fit, policy_test[,-response_index],type = "prob")
#Apply our decision tree to our test dataset using predict() function.
CrossTable(policy_test$ClaimFlag, claim_pred, prop.chisq = FALSE,
prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
#Use boosting to boost performance of our model
tree_boost <- C5.0(x = policy_train[,-response_index], y = policy_train[,c("ClaimFlag")], trials = 5)
summary(tree_boost)
#Apply our decision tree to our test dataset using predict() function.
claim_pred_boost <- predict(tree_boost, policy_test[,-response_index])
claim_prob_boost <- predict(tree_boost, policy_test[,-response_index],type = "prob")
policy_test <- cbind(policy_test,claim_pred,claim_prob)
policy_test <- cbind(policy_test,claim_pred_boost,claim_prob_boost)
CrossTable(policy_test$ClaimFlag, claim_pred_boost, prop.chisq = FALSE,
prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
#84% accuracy, however, model only correctly predicted 50 out of 940 loan defaults.
#This is where the model needs to be improved.
#Apply our decision tree to our test dataset using predict() function.
CrossTable(policy_test$ClaimFlag, claim_pred, prop.chisq = FALSE,
prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
claim_prob_boost
claim_prob
CrossTable(policy_test$ClaimFlag, claim_pred_boost, prop.chisq = FALSE,
prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
#Apply our decision tree to our test dataset using predict() function.
CrossTable(policy_test$ClaimFlag, claim_pred, prop.chisq = FALSE,
prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
CrossTable(policy_test$ClaimFlag, claim_pred_boost, prop.chisq = FALSE,
prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
##################
#Load Libraries#
##################
library(DBI)
library(RSQLite)
library(dplyr)
library(woeBinning)
library(ggplot2)
library(keras)
library(tensorflow)
library(recipes)
set.seed(101)
options(scipen=999)
'%!in%' <- function(x,y)!('%in%'(x,y))
#Source in the load_data function from the file data_pull.R.
#This keeps the main script (model_source.R clean and easy to read.
source('1_data_pull.R') #load_data(database) function which will pull data from SQLite DB
policy_data <- load_data_nn("admiral_db.db") #Returns list of two, first is train data, second is test.
policy_train <- as.data.frame(policy_data[1])
policy_test <- as.data.frame(policy_data[2])
(nrow(policy_train[policy_train$ClaimFlag == 1,])/nrow(policy_train))*100
(nrow(policy_test[policy_test$ClaimFlag == 1,])/nrow(policy_test))*100
str(policy_train)
##########################
#TUNE HYPERPARAMS#
##########################
#Test different variations of hyperparameters
#Set Hyperparams:
nodes1 = c(256, 512, 1024)
nodes2 = c(64, 128, 256, 1024)
dropout1 = c(0.4, 0.2)
dropout2 = c(0.2)
optimizer = c("adam")
lr_annealing = c(0.001)
batch_size = c(50)
epochs = c(20)
combinations <- length(nodes1) * length(nodes2) * length(dropout1) *
length(dropout2) * length(optimizer) * length(lr_annealing) * length(batch_size) * length(epochs)
#Test different variations of hyperparameters using model_tune function from model_source.R!
results_df <- data.frame()
predictions_df <- data.frame()
run_num <- 1
for (n1 in nodes1){
for (n2 in nodes2){
for(d1 in dropout1){
for (d2 in dropout2){
for (opt in optimizer){
for (lr in lr_annealing){
for (bs in batch_size){
for (e in epochs){
print(paste("Run number ",run_num,"/",combinations, sep=""))
print(paste(n1,n2,d1,d2,opt,lr,bs,e))
source('2_model_train_nn.R') #model_train(train_data) function which will train our train dataset.
result <- model_train(policy_train,policy_test,n1,n2,d1,d2,opt,bs,e, lr)
result_model <- as.data.frame(result[1]) #results of model
prediction <- as.data.frame(result[2]) #predictions
results_df <- rbind(results_df, result_model)
predictions_df <- rbind(predictions_df, prediction)
run_num <- run_num + 1
}
}
}
}
}
}
}
}
results_df <- results_df[order(-results_df$success_rate_claimers_25),]
opt_result <- results_df[1,]
opt_run_num <- opt_result$run_num[1]
claim_predictions <- predictions_df[predictions_df$run_num == opt_run_num,] #Select predictions of our optimal model (opt_run_number)
claim_predictions <- claim_predictions[order(-claim_predictions$actual, -claim_predictions$prob),]
##############
#RESULTS!
##############
#Using  50% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
#Using  25% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct_25 == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct_25 == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
##################
#Load Libraries#
##################
library(DBI)
library(RSQLite)
library(dplyr)
library(woeBinning)
library(ggplot2)
library(keras)
library(tensorflow)
library(recipes)
set.seed(101)
options(scipen=999)
'%!in%' <- function(x,y)!('%in%'(x,y))
#Source in the load_data function from the file data_pull.R.
#This keeps the main script (model_source.R clean and easy to read.
source('1_data_pull.R') #load_data(database) function which will pull data from SQLite DB
policy_data <- load_data_nn("admiral_db.db") #Returns list of two, first is train data, second is test.
policy_train <- as.data.frame(policy_data[1])
policy_test <- as.data.frame(policy_data[2])
(nrow(policy_train[policy_train$ClaimFlag == 1,])/nrow(policy_train))*100
(nrow(policy_test[policy_test$ClaimFlag == 1,])/nrow(policy_test))*100
str(policy_train)
##########################
#TUNE HYPERPARAMS#
##########################
#Test different variations of hyperparameters
#Set Hyperparams:
nodes1 = c(256, 512, 1024)
nodes2 = c(64, 128, 256, 1024)
dropout1 = c(0.4, 0.2)
dropout2 = c(0.2)
optimizer = c("adam")
lr_annealing = c(0.001)
batch_size = c(50)
epochs = c(20)
combinations <- length(nodes1) * length(nodes2) * length(dropout1) *
length(dropout2) * length(optimizer) * length(lr_annealing) * length(batch_size) * length(epochs)
#Test different variations of hyperparameters using model_tune function from model_source.R!
results_df <- data.frame()
predictions_df <- data.frame()
run_num <- 1
for (n1 in nodes1){
for (n2 in nodes2){
for(d1 in dropout1){
for (d2 in dropout2){
for (opt in optimizer){
for (lr in lr_annealing){
for (bs in batch_size){
for (e in epochs){
print(paste("Run number ",run_num,"/",combinations, sep=""))
print(paste(n1,n2,d1,d2,opt,lr,bs,e))
source('2_model_train_nn.R') #model_train(train_data, test_data,n1,n2,d1,d2,opt,bs,e, lr) function which will train our train dataset.
result <- model_train(policy_train,policy_test,n1,n2,d1,d2,opt,bs,e, lr)
result_model <- as.data.frame(result[1]) #results of model
prediction <- as.data.frame(result[2]) #predictions
results_df <- rbind(results_df, result_model)
predictions_df <- rbind(predictions_df, prediction)
run_num <- run_num + 1
}
}
}
}
}
}
}
}
print("Tuning run finished")
#Order results_df by success_rate_claimers_25, this variables is a percentage of how many claim predictions we were correct on, out of all claims (ClaimFlag =1).
#We use this variables rather than the general success rate, because the majority of observations are non-claims (ClaimFlag=0) - 93%
#Therefore it is key to understanding how our model performed at predicting ClaimFLag=1 specifically.
results_df <- results_df[order(-results_df$success_rate_claimers_25),]
opt_result <- results_df[1,]
opt_run_num <- opt_result$run_num[1]
claim_predictions <- predictions_df[predictions_df$run_num == opt_run_num,] #Select predictions of our optimal model (opt_run_number)
claim_predictions <- claim_predictions[order(-claim_predictions$actual, -claim_predictions$prob),]
##############
#RESULTS!
##############
#Using  50% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
#Using  25% as a threshold - We use a lower threshold for classifying results because ClaimFlag=1 has a much smaller proportion (~7%), therefore predicting this value is much more difficult.
#The model will therefore be more likely to score observations lower, even though they are claims, hence, by using a lower threshold,
#we maintain the overall accuracy levels, whilst also increasing our claim success prediction massively.
#success rate
nrow(claim_predictions[claim_predictions$correct_25 == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct_25 == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
View(results_df)
#Set Hyperparams:
nodes1 = c(2512, 1024, 2048)
nodes2 = c(128, 256, 1024)
dropout1 = c(0.4, 0.2)
dropout2 = c(0.2)
optimizer = c("adam")
lr_annealing = c(0.001)
batch_size = c(50)
epochs = c(20)
combinations <- length(nodes1) * length(nodes2) * length(dropout1) *
length(dropout2) * length(optimizer) * length(lr_annealing) * length(batch_size) * length(epochs)
#Test different variations of hyperparameters using model_tune function from model_source.R!
results_df <- data.frame()
predictions_df <- data.frame()
run_num <- 1
for (n1 in nodes1){
for (n2 in nodes2){
for(d1 in dropout1){
for (d2 in dropout2){
for (opt in optimizer){
for (lr in lr_annealing){
for (bs in batch_size){
for (e in epochs){
print(paste("Run number ",run_num,"/",combinations, sep=""))
print(paste(n1,n2,d1,d2,opt,lr,bs,e))
source('2_model_train_nn.R') #model_train(train_data, test_data,n1,n2,d1,d2,opt,bs,e, lr) function which will train our train dataset.
result <- model_train(policy_train,policy_test,n1,n2,d1,d2,opt,bs,e, lr)
result_model <- as.data.frame(result[1]) #results of model
prediction <- as.data.frame(result[2]) #predictions
results_df <- rbind(results_df, result_model)
predictions_df <- rbind(predictions_df, prediction)
run_num <- run_num + 1
}
}
}
}
}
}
}
}
#Set Hyperparams:
nodes1 = c(1024, 2048)
nodes2 = c(256, 512, 2024)
dropout1 = c(0.4)
dropout2 = c(0.2)
optimizer = c("adam")
lr_annealing = c(0.001)
batch_size = c(50)
epochs = c(20)
combinations <- length(nodes1) * length(nodes2) * length(dropout1) *
length(dropout2) * length(optimizer) * length(lr_annealing) * length(batch_size) * length(epochs)
#Test different variations of hyperparameters using model_tune function from model_source.R!
results_df <- data.frame()
predictions_df <- data.frame()
run_num <- 1
for (n1 in nodes1){
for (n2 in nodes2){
for(d1 in dropout1){
for (d2 in dropout2){
for (opt in optimizer){
for (lr in lr_annealing){
for (bs in batch_size){
for (e in epochs){
print(paste("Run number ",run_num,"/",combinations, sep=""))
print(paste(n1,n2,d1,d2,opt,lr,bs,e))
source('2_model_train_nn.R') #model_train(train_data, test_data,n1,n2,d1,d2,opt,bs,e, lr) function which will train our train dataset.
result <- model_train(policy_train,policy_test,n1,n2,d1,d2,opt,bs,e, lr)
result_model <- as.data.frame(result[1]) #results of model
prediction <- as.data.frame(result[2]) #predictions
results_df <- rbind(results_df, result_model)
predictions_df <- rbind(predictions_df, prediction)
run_num <- run_num + 1
}
}
}
}
}
}
}
}
print("Tuning run finished")
#Order results_df by success_rate_claimers_25, this variables is a percentage of how many claim predictions we were correct on, out of all claims (ClaimFlag =1).
#We use this variables rather than the general success rate, because the majority of observations are non-claims (ClaimFlag=0) - 93%
#Therefore it is key to understanding how our model performed at predicting ClaimFLag=1 specifically.
results_df <- results_df[order(-results_df$success_rate_claimers_25),]
opt_result <- results_df[1,]
opt_run_num <- opt_result$run_num[1]
claim_predictions <- predictions_df[predictions_df$run_num == opt_run_num,] #Select predictions of our optimal model (opt_run_number)
claim_predictions <- claim_predictions[order(-claim_predictions$actual, -claim_predictions$prob),]
#Using  50% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
#Using  25% as a threshold - We use a lower threshold for classifying results because ClaimFlag=1 has a much smaller proportion (~7%), therefore predicting this value is much more difficult.
#The model will therefore be more likely to score observations lower, even though they are claims, hence, by using a lower threshold,
#we maintain the overall accuracy levels, whilst also increasing our claim success prediction massively.
#success rate
nrow(claim_predictions[claim_predictions$correct_25 == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct_25 == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
View(results_df)
#Set Hyperparams:
nodes1 = c(1024, 2048)
nodes2 = c(256, 512)
dropout1 = c(0.2, 0.4)
dropout2 = c(0.2, 0.4)
optimizer = c("adam")
lr_annealing = c(0.001)
batch_size = c(50)
epochs = c(20)
combinations <- length(nodes1) * length(nodes2) * length(dropout1) *
length(dropout2) * length(optimizer) * length(lr_annealing) * length(batch_size) * length(epochs)
#Test different variations of hyperparameters using model_tune function from model_source.R!
results_df <- data.frame()
predictions_df <- data.frame()
run_num <- 1
for (n1 in nodes1){
for (n2 in nodes2){
for(d1 in dropout1){
for (d2 in dropout2){
for (opt in optimizer){
for (lr in lr_annealing){
for (bs in batch_size){
for (e in epochs){
print(paste("Run number ",run_num,"/",combinations, sep=""))
print(paste(n1,n2,d1,d2,opt,lr,bs,e))
source('2_model_train_nn.R') #model_train(train_data, test_data,n1,n2,d1,d2,opt,bs,e, lr) function which will train our train dataset.
result <- model_train(policy_train,policy_test,n1,n2,d1,d2,opt,bs,e, lr)
result_model <- as.data.frame(result[1]) #results of model
prediction <- as.data.frame(result[2]) #predictions
results_df <- rbind(results_df, result_model)
predictions_df <- rbind(predictions_df, prediction)
run_num <- run_num + 1
}
}
}
}
}
}
}
}
print("Tuning run finished")
#Order results_df by success_rate_claimers_25, this variables is a percentage of how many claim predictions we were correct on, out of all claims (ClaimFlag =1).
#We use this variables rather than the general success rate, because the majority of observations are non-claims (ClaimFlag=0) - 93%
#Therefore it is key to understanding how our model performed at predicting ClaimFLag=1 specifically.
results_df <- results_df[order(-results_df$success_rate_claimers_25),]
opt_result <- results_df[1,]
opt_run_num <- opt_result$run_num[1]
claim_predictions <- predictions_df[predictions_df$run_num == opt_run_num,] #Select predictions of our optimal model (opt_run_number)
claim_predictions <- claim_predictions[order(-claim_predictions$actual, -claim_predictions$prob),]
##############
#RESULTS!
##############
#Using  50% as a threshold
#success rate
nrow(claim_predictions[claim_predictions$correct == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
#Using  25% as a threshold - We use a lower threshold for classifying results because ClaimFlag=1 has a much smaller proportion (~7%), therefore predicting this value is much more difficult.
#The model will therefore be more likely to score observations lower, even though they are claims, hence, by using a lower threshold,
#we maintain the overall accuracy levels, whilst also increasing our claim success prediction massively.
#success rate
nrow(claim_predictions[claim_predictions$correct_25 == 1,])/nrow(claim_predictions)*100
#success_rate_claimers
(nrow(claim_predictions[claim_predictions$correct_25 == 1 & claim_predictions$actual == 1,])/nrow(claim_predictions[claim_predictions$actual == 1,]))*100
